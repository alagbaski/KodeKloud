#Countdown job in Kubernetes.
#Create a job "countdown-xfusion".
#The spec template should be named as countdown-xfusion (under metadata), and the container should be named as "container-countdown-xfusion".
#Use image "centos" with "latest" tag only and remember to mention tag, and restart policy should be "Never".
#Use command (for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done)


//SOLUTION:
#Create the job template as a .yaml manifest.
- The sample of the file can be found in this link --->  https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Jobs%20in%20Kubernets.yaml
#Apply the template.
kubectl apply -f <file-name>.yaml
#Verify if job is created according to template.
kubectl get jobs
kubectl logs <job-name>
kubectl logs <pod-name>
kubectl get pods --selector=job-name=<job-name> --output=jsonpath={.items..metadata.name}
- Replace <job-name> with the actual name of the job.
========================================================================================================================================================================================
#Deploy Node App on Kubernetes.
#Create a deployment using gcr.io/kodekloud/centos-ssh-enabled:node image, replica count must be 2.
#Create a service to expose this app, the service type must be NodePort, targetPort must be 8080 and nodePort should be 30012.
#Make sure all the pods are in Running state after the deployment.
#You can use any labels as per your choice.


//SOLUTION:
#To deploy the Node application on a Kubernetes cluster and meet the given requirements, you can use the following YAML configurations.
#Create a deployment.yaml & service.yaml manifest.
- Sample of the deployment file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Node%20App%20in%20Kubernetes/node_app_deployment.yaml
- Sample of the service file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Node%20App%20in%20Kubernetes/node_app_service.yaml
#Apply the configured YAML files in the following order.
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
#Verify if the application is running.
kubectl get pods
======================================================================================================================================================================================
#Pull Docker Image.
#Pull busybox:musl image on App Server 2 in Stratos DC and re-tag (create new tag) this image as busybox:local.


//SOLUTION:
#I noticed that you have to use elevated permission for each command.
#To pull the image
sudo docker pull busybox:musl
#To re-tag the image
sudo docker tag busybox:musl busybox:local
=======================================================================================================================================================================================
#Save, Load and Transfer Docker Image.
#On App Server 1 save the image "blog:nautilus" in an archive.
#Transfer the image archive to App Server 3.
#Load that image archive on App Server 3 with same name and tag which was used on App Server 1.


//SOLUTION:
#Connect  to App-Server 1 terminal using SSH.
#Create the image archive ".tar".
sudo docker save -o blog_nautilus.tar blog:nautilus
#Transfer file to App-Server 3.
sudo scp blog_nautilus.tar user@<stapp03-ip-address>:/home/user
#Make sure the docker service is up and running on the Servers.
sudo systemctl status docker
sudo systemctl start docker
- Run commands for both Servers.
#Load the image on App-Server 3.
sudo docker load -i blog_nautilus.tar
======================================================================================================================================================================================
#Deploy Guest Book App on Kubernetes.
=BACK-END TIER=
#Create a deployment named "redis-master" for Redis master.
- Replicas count should be 1.
- Container name should be "master-redis-devops" and it should use image "redis".
- Request resources as CPU should be "100m" and Memory should be "100Mi".
- Container port should be redis default port "6379".
#Create a service named "redis-master" for Redis master. Port and targetPort should be Redis default port "6379".
#Create another deployment named "redis-slave" for Redis slave.
- Replicas count should be 2.
- Container name should be "slave-redis-devops" and it should use "gcr.io/google_samples/gb-redisslave:v3" image.
- Requests resources as CPU should be "100m" and Memory should be "100Mi".
- Define an environment variable named "GET_HOSTS_FROM" and its value should be "dns".
- Container port should be Redis default port "6379".
#Create another service named "redis-slave". It should use Redis default port "6379".

=FRONT-END TIER=
#Create a deployment named frontend.
- Replicas count should be 3.
- Container name should be "php-redis-devops" and it should use "gcr.io/google-samples/gb-frontend:v4" image.
- Request resources as CPU should be "100m" and Memory should be "100Mi".
- Define an environment variable named as "GET_HOSTS_FROM" and its value should be "dns".
- Container port should be "80".
#Create a service named "frontend". Its type should be "NodePort", port should be "80" and its nodePort should be "30009".


//SOLUTION:
#We can create seprate .yaml manifest to create the deployment and services for each task.
#Make to execute the task accordingly.
- Sample of all the task manifest can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Deploy%20Guest%20Book%20App%20on%20Kubernetes.yaml
#Apply each configuration using the "kubectl apply -f <filename>.yaml".
#Also, we can create a single manifest file consisting of the other task execution just the way it is found in the link above. Just run this command instaed.
kubectl create -f <filename>.yaml
#Verify the task execution.
kubectl get deployment
kubectl get service
========================================================================================================================================================================================
#Docker Volumes Mapping.
#On App Server 2 in Stratos DC pull nginx image preferably latest tag.
#Create a new container with name cluster from the image you just pulled.
#Map the host volume /opt/security with container volume /usr/src/.
#There is an sample.txt file present on same server under /tmp; copy that file to /opt/security.


//SOLUTION:
#Connect to App Server 2 in the Stratos DC using SSH or any other remote access method.
#Pull the Nginx image from Docker Hub.
sudo docker pull nginx
#Create a new container named "cluster" using the Nginx image you just pulled.
sudo docker run -d --name cluster -v /opt/security:/usr/src/ nginx
#Copy the sample.txt file from the host server to the /opt/security directory.
sudo cp /tmp/sample.txt /opt/security
#Also please keep the container in running state.
sudo docker ps
=======================================================================================================================================================================================
#Ansible Ping Module Usage.
#Set up a password-less SSH connection between Ansible controller and Ansible managed nodes.
- Jump host is our Ansible controller, and we are going to run Ansible playbooks through thor user on jump host.
- Make appropriate changes on jump host so that user thor on jump host can SSH into App Server 3 through its respective sudo user.
#There is an inventory file /home/thor/ansible/inventory on jump host. Using that inventory file test Ansible ping from jump host to App Server 3, make sure ping works.


//SOLUTION:
#Firstly, Generate SSH Key Pair on the Ansible controller (Jump Host).
ssh-keygen -t rsa
- Press enter when prompt and leave passwpharse empty.
#Copy the Public Key to the Managed Node (App Server 3).
ssh-copy-id -i ~/.ssh/id_rsa.pub banner@ip-address
#Test SSH Connection from Jump Host to App Server 3.
ssh banner@ip-address
#Test Ansible Ping from Jump Host to App Server 3.
ansible -i /home/thor/ansible/inventory stapp03 -m ping
===================================================================================================================================================================================
#Rollback a Deployment in Kubernetes.
#There is a deployment named "nginx-deployment"; roll it back to the previous revision.


//SOLUTION:
#Navigate into the Kubernetes Utility Server.
#Run the command to rollback the deployment.
kubectl rollout undo deployment/nginx-deployment
#Verify if the deployment has been rolled back.
kubectl rollout status deployment/nginx-deployment
===============================================================================================================================================================================
#Puppet Install a Package.
#Some new packages need to be installed on app server 3 in Stratos Datacenter.
#Create a Puppet programming file demo.pp under /etc/puppetlabs/code/environments/production/manifests directory.
#Install package httpd through Puppet package resource only on App server 3.


//SOLUTION:
#Navigate to the manifests directory on the Puppet Server and Create the puppet file named "demo.pp".
cd /etc/puppetlabs/code/enviroments/production/manifests
sudo vi demo.pp
- You can find the sample of the file in this link ---> https://github.com/mightygull/Puppet-Files/blob/main/Puppet%20Install%20Package%20On%20Agent.pp
#Make sure to run the puppet agent test using 'sudo' on agent nodes.
#Apply the Manifest on App Server 3.
sudo puppet agent -t
#Verify Installation.
sudo systemctl status httpd
#Check if the Puppet server and agent nodes are running, you can use the following commands on the respective servers:
sudo systemctl status puppetserver
sudo systemctl status puppet
==============================================================================================================================================================================
#Ansible Unarchive Module.
#We have an inventory file under /home/thor/ansible directory on jump host, which should have all the app servers added already.
#There is a ZIP archive "/usr/src/security/xfusion.zip" on jump host.
- Create a playbook.yml under "/home/thor/ansible/" directory on jump host itself to perform the below given tasks.
- Unzip "/usr/src/security/xfusion.zip" archive in "/opt/security/" location on all app servers.
#Make sure the extracted data must have the respective sudo user as their "user" and "group" owner, the extracted data permissions must be "0644".
#Validation will try to run the playbook using command "ansible-playbook -i inventory playbook.yml" so please make sure playbook works this way, without passing any extra arguments.


//SOLUTION:
#Navigate to the ansible directory where the inventory file is present and Create the playbook.yml to carry out the task requirement.
cd ansible/
vi playbook.yml
- Sample of the playbook.yml file can be found in this link ---> https://github.com/mightygull/Ansible-Playbooks/blob/main/Ansible%20Unarchive%20Module.yml
#Validate the playbook.yml to run on all the target servers.
ansible-playbook -i inventory playbook.yml
#To verify the task execution, you can SSH into each target Server to verify that the task required was executed successfully.
==================================================================================================================================================================================
