#Countdown job in Kubernetes.
#Create a job "countdown-xfusion".
#The spec template should be named as countdown-xfusion (under metadata), and the container should be named as "container-countdown-xfusion".
#Use image "centos" with "latest" tag only and remember to mention tag, and restart policy should be "Never".
#Use command (for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done)


//SOLUTION:
#Create the job template as a .yaml manifest.
- The sample of the file can be found in this link --->  https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Jobs%20in%20Kubernets.yaml
#Apply the template.
kubectl apply -f <file-name>.yaml
#Verify if job is created according to template.
kubectl get jobs
kubectl logs <job-name>
kubectl logs <pod-name>
kubectl get pods --selector=job-name=<job-name> --output=jsonpath={.items..metadata.name}
- Replace <job-name> with the actual name of the job.
========================================================================================================================================================================================
#Deploy Node App on Kubernetes.
#Create a deployment using gcr.io/kodekloud/centos-ssh-enabled:node image, replica count must be 2.
#Create a service to expose this app, the service type must be NodePort, targetPort must be 8080 and nodePort should be 30012.
#Make sure all the pods are in Running state after the deployment.
#You can use any labels as per your choice.


//SOLUTION:
#To deploy the Node application on a Kubernetes cluster and meet the given requirements, you can use the following YAML configurations.
#Create a deployment.yaml & service.yaml manifest.
- Sample of the deployment file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Node%20App%20in%20Kubernetes/node_app_deployment.yaml
- Sample of the service file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Node%20App%20in%20Kubernetes/node_app_service.yaml
#Apply the configured YAML files in the following order.
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
#Verify if the application is running.
kubectl get pods
======================================================================================================================================================================================
#Pull Docker Image.
#Pull busybox:musl image on App Server 2 in Stratos DC and re-tag (create new tag) this image as busybox:local.


//SOLUTION:
#I noticed that you have to use elevated permission for each command.
#To pull the image
sudo docker pull busybox:musl
#To re-tag the image
sudo docker tag busybox:musl busybox:local
=======================================================================================================================================================================================
#Save, Load and Transfer Docker Image.
#On App Server 1 save the image "blog:nautilus" in an archive.
#Transfer the image archive to App Server 3.
#Load that image archive on App Server 3 with same name and tag which was used on App Server 1.


//SOLUTION:
#Connect  to App-Server 1 terminal using SSH.
#Create the image archive ".tar".
sudo docker save -o blog_nautilus.tar blog:nautilus
#Transfer file to App-Server 3.
sudo scp blog_nautilus.tar user@<stapp03-ip-address>:/home/user
#Make sure the docker service is up and running on the Servers.
sudo systemctl status docker
sudo systemctl start docker
- Run commands for both Servers.
#Load the image on App-Server 3.
sudo docker load -i blog_nautilus.tar
======================================================================================================================================================================================
#Deploy Guest Book App on Kubernetes.
=BACK-END TIER=
#Create a deployment named "redis-master" for Redis master.
- Replicas count should be 1.
- Container name should be "master-redis-devops" and it should use image "redis".
- Request resources as CPU should be "100m" and Memory should be "100Mi".
- Container port should be redis default port "6379".
#Create a service named "redis-master" for Redis master. Port and targetPort should be Redis default port "6379".
#Create another deployment named "redis-slave" for Redis slave.
- Replicas count should be 2.
- Container name should be "slave-redis-devops" and it should use "gcr.io/google_samples/gb-redisslave:v3" image.
- Requests resources as CPU should be "100m" and Memory should be "100Mi".
- Define an environment variable named "GET_HOSTS_FROM" and its value should be "dns".
- Container port should be Redis default port "6379".
#Create another service named "redis-slave". It should use Redis default port "6379".

=FRONT-END TIER=
#Create a deployment named frontend.
- Replicas count should be 3.
- Container name should be "php-redis-devops" and it should use "gcr.io/google-samples/gb-frontend:v4" image.
- Request resources as CPU should be "100m" and Memory should be "100Mi".
- Define an environment variable named as "GET_HOSTS_FROM" and its value should be "dns".
- Container port should be "80".
#Create a service named "frontend". Its type should be "NodePort", port should be "80" and its nodePort should be "30009".


//SOLUTION:
#We can create seprate .yaml manifest to create the deployment and services for each task.
#Make to execute the task accordingly.
- Sample of all the task manifest can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Deploy%20Guest%20Book%20App%20on%20Kubernetes.yaml
#Apply each configuration using the "kubectl apply -f <filename>.yaml".
#Also, we can create a single manifest file consisting of the other task execution just the way it is found in the link above. Just run this command instaed.
kubectl create -f <filename>.yaml
#Verify the task execution.
kubectl get deployment
kubectl get service
========================================================================================================================================================================================
#Docker Volumes Mapping.
#On App Server 2 in Stratos DC pull nginx image preferably latest tag.
#Create a new container with name cluster from the image you just pulled.
#Map the host volume /opt/security with container volume /usr/src/.
#There is an sample.txt file present on same server under /tmp; copy that file to /opt/security.


//SOLUTION:
#Connect to App Server 2 in the Stratos DC using SSH or any other remote access method.
#Pull the Nginx image from Docker Hub.
sudo docker pull nginx
#Create a new container named "cluster" using the Nginx image you just pulled.
sudo docker run -d --name cluster -v /opt/security:/usr/src/ nginx
#Copy the sample.txt file from the host server to the /opt/security directory.
sudo cp /tmp/sample.txt /opt/security
#Also please keep the container in running state.
sudo docker ps
=======================================================================================================================================================================================
#Ansible Ping Module Usage.
#Set up a password-less SSH connection between Ansible controller and Ansible managed nodes.
- Jump host is our Ansible controller, and we are going to run Ansible playbooks through thor user on jump host.
- Make appropriate changes on jump host so that user thor on jump host can SSH into App Server 3 through its respective sudo user.
#There is an inventory file /home/thor/ansible/inventory on jump host. Using that inventory file test Ansible ping from jump host to App Server 3, make sure ping works.


//SOLUTION:
#Firstly, Generate SSH Key Pair on the Ansible controller (Jump Host).
ssh-keygen -t rsa
- Press enter when prompt and leave passwpharse empty.
#Copy the Public Key to the Managed Node (App Server 3).
ssh-copy-id -i ~/.ssh/id_rsa.pub banner@ip-address
#Test SSH Connection from Jump Host to App Server 3.
ssh banner@ip-address
#Test Ansible Ping from Jump Host to App Server 3.
ansible -i /home/thor/ansible/inventory stapp03 -m ping
===================================================================================================================================================================================
#Rollback a Deployment in Kubernetes.
#There is a deployment named "nginx-deployment"; roll it back to the previous revision.


//SOLUTION:
#Navigate into the Kubernetes Utility Server.
#Run the command to rollback the deployment.
kubectl rollout undo deployment/nginx-deployment
#Verify if the deployment has been rolled back.
kubectl rollout status deployment/nginx-deployment
===============================================================================================================================================================================
#Puppet Install a Package.
#Some new packages need to be installed on app server 3 in Stratos Datacenter.
#Create a Puppet programming file demo.pp under /etc/puppetlabs/code/environments/production/manifests directory.
#Install package httpd through Puppet package resource only on App server 3.


//SOLUTION:
#Navigate to the manifests directory on the Puppet Server and Create the puppet file named "demo.pp".
cd /etc/puppetlabs/code/enviroments/production/manifests
sudo vi demo.pp
- You can find the sample of the file in this link ---> https://github.com/mightygull/Puppet-Files/blob/main/Puppet%20Install%20Package%20On%20Agent.pp
#Make sure to run the puppet agent test using 'sudo' on agent nodes.
#Apply the Manifest on App Server 3.
sudo puppet agent -t
#Verify Installation.
sudo systemctl status httpd
#Check if the Puppet server and agent nodes are running, you can use the following commands on the respective servers:
sudo systemctl status puppetserver
sudo systemctl status puppet
==============================================================================================================================================================================
#Ansible Unarchive Module.
#We have an inventory file under /home/thor/ansible directory on jump host, which should have all the app servers added already.
#There is a ZIP archive "/usr/src/security/xfusion.zip" on jump host.
- Create a playbook.yml under "/home/thor/ansible/" directory on jump host itself to perform the below given tasks.
- Unzip "/usr/src/security/xfusion.zip" archive in "/opt/security/" location on all app servers.
#Make sure the extracted data must have the respective sudo user as their "user" and "group" owner, the extracted data permissions must be "0644".
#Validation will try to run the playbook using command "ansible-playbook -i inventory playbook.yml" so please make sure playbook works this way, without passing any extra arguments.


//SOLUTION:
#Navigate to the ansible directory where the inventory file is present and Create the playbook.yml to carry out the task requirement.
cd ansible/
vi playbook.yml
- Sample of the playbook.yml file can be found in this link ---> https://github.com/mightygull/Ansible-Playbooks/blob/main/Ansible%20Unarchive%20Module.yml
#Validate the playbook.yml to run on all the target servers.
ansible-playbook -i inventory playbook.yml
#To verify the task execution, you can SSH into each target Server to verify that the task required was executed successfully.
==================================================================================================================================================================================
#Kubernetes Troubleshooting.
#One of the Nautilus DevOps team members was working on to update an existing Kubernetes template. Somehow, he made some mistakes in the template and it is failing while applying.
#Also, do not remove any component from the template like pods/deployments/volumes etc.
#"/home/thor/mysql_deployment.yml" is the template that needs to be fixed.


//SOLUTION:
#Navigate to the directory where the .yml file is
cd /home/thor
#To run a test on the .yml in kubernetes server
kubectl apply --dry-run=client -f /path/to/filename.yml
- We assume the output listed what could be wrong with the file.
- There is a sample of the correct .yml in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Kubernetes%20Troubleshooting.yml
#Run the test on the .yml file after necessary modifications.
kubectl apply --dry-run=client -f /path/to/filename.yml
kubectl apply -f filename.yml
#Verify you task execution.
kubectl get svc
kubectl get deployment
kubectl get pv
kubectl get pvc
kubectl get pods
=====================================================================================================================================================================================
#Deploy MySQL on Kubernetes
#Create a PersistentVolume "mysql-pv", its capacity should be "250Mi", set other parameters as per your preference.
#Create a PersistentVolumeClaim to request this PersistentVolume storage. Name it as "mysql-pv-claim" and request a "250Mi" of storage. Set other parameters as per your preference.
#Create a deployment named "mysql-deployment", use any mysql image as per your preference. Mount the PersistentVolume at mount path "/var/lib/mysql".
#Create a NodePort type service named "mysql" and set nodePort to "30007".
#Create a secret named "mysql-root-pass" having a key pair value.
- Where key is "password" and its value is "YUIidhb667".
#Create another secret named "mysql-user-pass" having some key pair values.
- Where fiRst key is "username" and its value is "kodekloud_joy", second key is "password" and value is "B4zNgHA7Ya".
#Create one more secret named "mysql-db-url".
- Where key name is "database" and value is "kodekloud_db10".
#Define some Environment variables within the container:
- name: MYSQL_ROOT_PASSWORD, should pick value from secretKeyRef name: mysql-root-pass and key: password.
- name: MYSQL_DATABASE, should pick value from secretKeyRef name: mysql-db-url and key: database.
- name: MYSQL_USER, should pick value from secretKeyRef name: mysql-user-pass key key: username.
- name: MYSQL_PASSWORD, should pick value from secretKeyRef name: mysql-user-pass and key: password.


//SOLUTION:
#Create .yaml manifest files as per task requirments.
vi secrets.yaml
vi pv.yaml
vi pvc.yaml
vi deployment.yaml
vi service.yaml
#Sample of the files can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/tree/main/Deploy%20MySQL%20on%20Kubernetes
#Apply the manifest files in this order.
kubectl apply -f secrets.yaml
kubectl apply -f pv.yaml
kubectl apply -f pvc.yaml
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
#Verify the task execution.
kubectl get pods
kubectl get deployment
kubectl get secrets
kubectl get svc
kubectl get pv
kubectl get pvc
=======================================================================================================================================================================================
#Git Manage Remotes.
#The xFusionCorp development team added updates to the project that is maintained under /opt/games.git repo and cloned under /usr/src/kodekloudrepos/games.
#The DevOps team added some new Git remotes, so we need to update remote on /usr/src/kodekloudrepos/games repository as per details mentioned below:
- In "/usr/src/kodekloudrepos/games" repo add a new remote "dev_games" and point it to "/opt/xfusioncorp_games.git" repository.
- There is a file "/tmp/index.html" on same server; copy this file to the repo and add/commit to "master" branch.
- Finally push master branch to this new remote origin.


//SOLUTION:
#SSH into the required Server in the Startos DC.
#Navigate to the games repository and add a new remote named "dev_games".
cd /usr/src/kodekloudrepos/games
sudo git remote add dev_games /opt/xfusioncorp_games.git
#Copy the file "/tmp/index.html" to the repository. Add and also commit it to the master branch.
sudo cp /tmp/index.html .
sudo git add index.html
sudo git commit -m "Added index.html from /tmp"
#Verify the commit before pushing.
git log
#Push the master branch to the new remote "origin".
sudo git push dev_games master
===================================================================================================================================================================================
