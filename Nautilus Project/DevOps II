#Countdown job in Kubernetes.
#Create a job "countdown-xfusion".
#The spec template should be named as countdown-xfusion (under metadata), and the container should be named as "container-countdown-xfusion".
#Use image "centos" with "latest" tag only and remember to mention tag, and restart policy should be "Never".
#Use command (for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done)


//SOLUTION:
#Create the job template as a .yaml manifest.
- The sample of the file can be found in this link --->  https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Jobs%20in%20Kubernets.yaml
#Apply the template.
kubectl apply -f <file-name>.yaml
#Verify if job is created according to template.
kubectl get jobs
kubectl logs <job-name>
kubectl logs <pod-name>
kubectl get pods --selector=job-name=<job-name> --output=jsonpath={.items..metadata.name}
- Replace <job-name> with the actual name of the job.
========================================================================================================================================================================================
#Deploy Node App on Kubernetes.
#Create a deployment using gcr.io/kodekloud/centos-ssh-enabled:node image, replica count must be 2.
#Create a service to expose this app, the service type must be NodePort, targetPort must be 8080 and nodePort should be 30012.
#Make sure all the pods are in Running state after the deployment.
#You can use any labels as per your choice.


//SOLUTION:
#To deploy the Node application on a Kubernetes cluster and meet the given requirements, you can use the following YAML configurations.
#Create a deployment.yaml & service.yaml manifest.
- Sample of the deployment file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Node%20App%20in%20Kubernetes/node_app_deployment.yaml
- Sample of the service file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Node%20App%20in%20Kubernetes/node_app_service.yaml
#Apply the configured YAML files in the following order.
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
#Verify if the application is running.
kubectl get pods
======================================================================================================================================================================================
#Pull Docker Image.
#Pull busybox:musl image on App Server 2 in Stratos DC and re-tag (create new tag) this image as busybox:local.


//SOLUTION:
#I noticed that you have to use elevated permission for each command.
#To pull the image
sudo docker pull busybox:musl
#To re-tag the image
sudo docker tag busybox:musl busybox:local
=======================================================================================================================================================================================
#Save, Load and Transfer Docker Image.
#On App Server 1 save the image "blog:nautilus" in an archive.
#Transfer the image archive to App Server 3.
#Load that image archive on App Server 3 with same name and tag which was used on App Server 1.


//SOLUTION:
#Connect  to App-Server 1 terminal using SSH.
#Create the image archive ".tar".
sudo docker save -o blog_nautilus.tar blog:nautilus
#Transfer file to App-Server 3.
sudo scp blog_nautilus.tar user@<stapp03-ip-address>:/home/user
#Make sure the docker service is up and running on the Servers.
sudo systemctl status docker
sudo systemctl start docker
- Run commands for both Servers.
#Load the image on App-Server 3.
sudo docker load -i blog_nautilus.tar
======================================================================================================================================================================================
#Deploy Guest Book App on Kubernetes.
=BACK-END TIER=
#Create a deployment named "redis-master" for Redis master.
- Replicas count should be 1.
- Container name should be "master-redis-devops" and it should use image "redis".
- Request resources as CPU should be "100m" and Memory should be "100Mi".
- Container port should be redis default port "6379".
#Create a service named "redis-master" for Redis master. Port and targetPort should be Redis default port "6379".
#Create another deployment named "redis-slave" for Redis slave.
- Replicas count should be 2.
- Container name should be "slave-redis-devops" and it should use "gcr.io/google_samples/gb-redisslave:v3" image.
- Requests resources as CPU should be "100m" and Memory should be "100Mi".
- Define an environment variable named "GET_HOSTS_FROM" and its value should be "dns".
- Container port should be Redis default port "6379".
#Create another service named "redis-slave". It should use Redis default port "6379".

=FRONT-END TIER=
#Create a deployment named frontend.
- Replicas count should be 3.
- Container name should be "php-redis-devops" and it should use "gcr.io/google-samples/gb-frontend:v4" image.
- Request resources as CPU should be "100m" and Memory should be "100Mi".
- Define an environment variable named as "GET_HOSTS_FROM" and its value should be "dns".
- Container port should be "80".
#Create a service named "frontend". Its type should be "NodePort", port should be "80" and its nodePort should be "30009".


//SOLUTION:
#We can create seprate .yaml manifest to create the deployment and services for each task.
#Make to execute the task accordingly.
- Sample of all the task manifest can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Deploy%20Guest%20Book%20App%20on%20Kubernetes.yaml
#Apply each configuration using the "kubectl apply -f <filename>.yaml".
#Also, we can create a single manifest file consisting of the other task execution just the way it is found in the link above. Just run this command instaed.
kubectl create -f <filename>.yaml
#Verify the task execution.
kubectl get deployment
kubectl get service
========================================================================================================================================================================================
#Docker Volumes Mapping.
#On App Server 2 in Stratos DC pull nginx image preferably latest tag.
#Create a new container with name cluster from the image you just pulled.
#Map the host volume /opt/security with container volume /usr/src/.
#There is an sample.txt file present on same server under /tmp; copy that file to /opt/security.


//SOLUTION:
#Connect to App Server 2 in the Stratos DC using SSH or any other remote access method.
#Pull the Nginx image from Docker Hub.
sudo docker pull nginx
#Create a new container named "cluster" using the Nginx image you just pulled.
sudo docker run -d --name cluster -v /opt/security:/usr/src/ nginx
#Copy the sample.txt file from the host server to the /opt/security directory.
sudo cp /tmp/sample.txt /opt/security
#Also please keep the container in running state.
sudo docker ps
=======================================================================================================================================================================================
#Ansible Ping Module Usage.
#Set up a password-less SSH connection between Ansible controller and Ansible managed nodes.
- Jump host is our Ansible controller, and we are going to run Ansible playbooks through thor user on jump host.
- Make appropriate changes on jump host so that user thor on jump host can SSH into App Server 3 through its respective sudo user.
#There is an inventory file /home/thor/ansible/inventory on jump host. Using that inventory file test Ansible ping from jump host to App Server 3, make sure ping works.


//SOLUTION:
#Firstly, Generate SSH Key Pair on the Ansible controller (Jump Host).
ssh-keygen -t rsa
- Press enter when prompt and leave passwpharse empty.
#Copy the Public Key to the Managed Node (App Server 3).
ssh-copy-id -i ~/.ssh/id_rsa.pub banner@ip-address
#Test SSH Connection from Jump Host to App Server 3.
ssh banner@ip-address
#Test Ansible Ping from Jump Host to App Server 3.
ansible -i /home/thor/ansible/inventory stapp03 -m ping
===================================================================================================================================================================================
#Rollback a Deployment in Kubernetes.
#There is a deployment named "nginx-deployment"; roll it back to the previous revision.


//SOLUTION:
#Navigate into the Kubernetes Utility Server.
#Run the command to rollback the deployment.
kubectl rollout undo deployment/nginx-deployment
#Verify if the deployment has been rolled back.
kubectl rollout status deployment/nginx-deployment
===============================================================================================================================================================================
#Puppet Install a Package.
#Some new packages need to be installed on app server 3 in Stratos Datacenter.
#Create a Puppet programming file demo.pp under /etc/puppetlabs/code/environments/production/manifests directory.
#Install package httpd through Puppet package resource only on App server 3.


//SOLUTION:
#Navigate to the manifests directory on the Puppet Server and Create the puppet file named "demo.pp".
cd /etc/puppetlabs/code/enviroments/production/manifests
sudo vi demo.pp
- You can find the sample of the file in this link ---> https://github.com/mightygull/Puppet-Files/blob/main/Puppet%20Install%20Package%20On%20Agent.pp
#Make sure to run the puppet agent test using 'sudo' on agent nodes.
#Apply the Manifest on App Server 3.
sudo puppet agent -t
#Verify Installation.
sudo systemctl status httpd
#Check if the Puppet server and agent nodes are running, you can use the following commands on the respective servers:
sudo systemctl status puppetserver
sudo systemctl status puppet
==============================================================================================================================================================================
#Ansible Unarchive Module.
#We have an inventory file under /home/thor/ansible directory on jump host, which should have all the app servers added already.
#There is a ZIP archive "/usr/src/security/xfusion.zip" on jump host.
- Create a playbook.yml under "/home/thor/ansible/" directory on jump host itself to perform the below given tasks.
- Unzip "/usr/src/security/xfusion.zip" archive in "/opt/security/" location on all app servers.
#Make sure the extracted data must have the respective sudo user as their "user" and "group" owner, the extracted data permissions must be "0644".
#Validation will try to run the playbook using command "ansible-playbook -i inventory playbook.yml" so please make sure playbook works this way, without passing any extra arguments.


//SOLUTION:
#Navigate to the ansible directory where the inventory file is present and Create the playbook.yml to carry out the task requirement.
cd ansible/
vi playbook.yml
- Sample of the playbook.yml file can be found in this link ---> https://github.com/mightygull/Ansible-Playbooks/blob/main/Ansible%20Unarchive%20Module.yml
#Validate the playbook.yml to run on all the target servers.
ansible-playbook -i inventory playbook.yml
#To verify the task execution, you can SSH into each target Server to verify that the task required was executed successfully.
==================================================================================================================================================================================
#Kubernetes Troubleshooting.
#One of the Nautilus DevOps team members was working on to update an existing Kubernetes template. Somehow, he made some mistakes in the template and it is failing while applying.
#Also, do not remove any component from the template like pods/deployments/volumes etc.
#"/home/thor/mysql_deployment.yml" is the template that needs to be fixed.


//SOLUTION:
#Navigate to the directory where the .yml file is
cd /home/thor
#To run a test on the .yml in kubernetes server
kubectl apply --dry-run=client -f /path/to/filename.yml
- We assume the output listed what could be wrong with the file.
- There is a sample of the correct .yml in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Kubernetes%20Troubleshooting.yml
#Run the test on the .yml file after necessary modifications.
kubectl apply --dry-run=client -f /path/to/filename.yml
kubectl apply -f filename.yml
#Verify you task execution.
kubectl get svc
kubectl get deployment
kubectl get pv
kubectl get pvc
kubectl get pods
=====================================================================================================================================================================================
#Deploy MySQL on Kubernetes
#Create a PersistentVolume "mysql-pv", its capacity should be "250Mi", set other parameters as per your preference.
#Create a PersistentVolumeClaim to request this PersistentVolume storage. Name it as "mysql-pv-claim" and request a "250Mi" of storage. Set other parameters as per your preference.
#Create a deployment named "mysql-deployment", use any mysql image as per your preference. Mount the PersistentVolume at mount path "/var/lib/mysql".
#Create a NodePort type service named "mysql" and set nodePort to "30007".
#Create a secret named "mysql-root-pass" having a key pair value.
- Where key is "password" and its value is "YUIidhb667".
#Create another secret named "mysql-user-pass" having some key pair values.
- Where fiRst key is "username" and its value is "kodekloud_joy", second key is "password" and value is "B4zNgHA7Ya".
#Create one more secret named "mysql-db-url".
- Where key name is "database" and value is "kodekloud_db10".
#Define some Environment variables within the container:
- name: MYSQL_ROOT_PASSWORD, should pick value from secretKeyRef name: mysql-root-pass and key: password.
- name: MYSQL_DATABASE, should pick value from secretKeyRef name: mysql-db-url and key: database.
- name: MYSQL_USER, should pick value from secretKeyRef name: mysql-user-pass key key: username.
- name: MYSQL_PASSWORD, should pick value from secretKeyRef name: mysql-user-pass and key: password.


//SOLUTION:
#Create .yaml manifest files as per task requirments.
vi secrets.yaml
vi pv.yaml
vi pvc.yaml
vi deployment.yaml
vi service.yaml
#Sample of the files can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/tree/main/Deploy%20MySQL%20on%20Kubernetes
#Apply the manifest files in this order.
kubectl apply -f secrets.yaml
kubectl apply -f pv.yaml
kubectl apply -f pvc.yaml
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
#Verify the task execution.
kubectl get pods
kubectl get deployment
kubectl get secrets
kubectl get svc
kubectl get pv
kubectl get pvc
=======================================================================================================================================================================================
#Git Manage Remotes.
#The xFusionCorp development team added updates to the project that is maintained under /opt/games.git repo and cloned under /usr/src/kodekloudrepos/games.
#The DevOps team added some new Git remotes, so we need to update remote on /usr/src/kodekloudrepos/games repository as per details mentioned below:
- In "/usr/src/kodekloudrepos/games" repo add a new remote "dev_games" and point it to "/opt/xfusioncorp_games.git" repository.
- There is a file "/tmp/index.html" on same server; copy this file to the repo and add/commit to "master" branch.
- Finally push master branch to this new remote origin.


//SOLUTION:
#SSH into the required Server in the Startos DC.
#Navigate to the games repository and add a new remote named "dev_games".
cd /usr/src/kodekloudrepos/games
sudo git remote add dev_games /opt/xfusioncorp_games.git
#Copy the file "/tmp/index.html" to the repository. Add and also commit it to the master branch.
sudo cp /tmp/index.html .
sudo git add index.html
sudo git commit -m "Added index.html from /tmp"
#Verify the commit before pushing.
git log
#Push the master branch to the new remote "origin".
sudo git push dev_games master
===================================================================================================================================================================================
#Install Docker Package.
#Install "docker-ce" and "docker-compose" packages on App Server 3.
#Start docker service.


//SOLUTION:
#Update the package index and install some necessary dependencies.
sudo yum update
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
#Add the Docker repository to the system and install Docker CE, start and enable it as well.
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install -y docker-ce
sudo systemctl start docker
sudo systemctl enable docker
#Install required dependencies for Docker Compose.
sudo yum install -y curl
#Download the latest version of Docker Compose (you can check for the latest release on the Docker Compose GitHub page).
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
#Apply executable permissions to the binary.
sudo chmod +x /usr/local/bin/docker-compose
#Create a symbolic link to make the docker-compose command available system-wide.
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
#Manage Docker without sudo (Optional).
sudo usermod -aG docker your_username
=====================================================================================================================================================================================
#Run a Docker Container.
#On Application Server 1 create a container named "nginx_1" using image "nginx" with "alpine" tag and make sure container is in running state.


//SOLUTION:
#Make sure docker is installed and running.
sudo systemctl status docker
docker ps -a
#Pull the Nginx image with the alpine tag from Docker Hub.
docker pull nginx:alpine
#Now create and run the Nginx container named "nginx_1".
docker run -d --name nginx_1 -p 80:80 nginx:alpine
#Verify if the container is running.
docker ps
====================================================================================================================================================================================
#Docker Delete Container.
#Delete a container named kke-container, its running on App Server 3 in Stratos DC.


//SOLUTION:
#First verify if container is in running state.
docker ps
#If its in running state it's best to stop it before deleting it.
docker stop kke-container
docker rm kke-container
#Verify if container is deleted.
docker ps -a
=====================================================================================================================================================================================
#Docker Container Issue.
#There is a static website running within a container named "nautilus", this container is on App Server 1.
#Suddenly, we started facing some issues with the static website on App Server 1. Look into the issue to fix the same, you can find more details below:
- Container's volume "/usr/local/apache2/htdocs" is mapped with the host's volume "/var/www/html".
- The website should run on host port 8080 on App Server 1 i.e command curl http://localhost:8080/ should work on App Server 1.


//SOLUTION:
#There are a list of things to do to troubleshoot issues
- Check if the Container is Running.
- Verify the Volume Mapping.
- Verify the Website's Content.
- Check Port Mapping.
- Test the Website.

#To check if container is running
docker ps
docker ps -a [Incase the container is not running, Run: "docker start nautilus"]

#To verify Volume Mapping.
docker inspect nautilus

#To verify website content.
- Check the content of the static website on the host by navigating to the directory /var/www/html and verifying the HTML files and folders.

#To check port mapping.
docker ps -f nautilus --format "{{.Names}}\t{{.Ports}}"
- it should give you an output like this "nautilus   0.0.0.0:8080->80/tcp"

#To test the website.
curl http://localhost:8080
====================================================================================================================================================================================
#Ansible Basic Playbook.
#The playbook needs to be run on App Server 2 in Stratos DC, so inventory file needs to be updated accordingly.
#The inventory file /home/thor/ansible/inventory seems to be having some issues.
- Create a playbook "/home/thor/ansible/playbook.yml" and add a task to create an empty file "/tmp/file.txt" on App Server 2.
- Run the playbook using command "ansible-playbook -i inventory playbook.yml" so please make sure the playbook works this way without passing any extra arguments.


//SOLUTION:
#First resolve the issue with the inventory file.
- Make sure the inventory file is defined like this.
stapp02 ansible_host=ip-address ansible_ssh_pass=password ansible_user=username
#Navigate to the /home/thor/ansible/ directory to create the playbook.yml
cd /home/thor/ansible/
vi playbook.yml
- Sample of the file can be found in this link ---> https://github.com/mightygull/Ansible-Playbooks/blob/main/create%20an%20empty%20file.yml
#Run the Playbook.
ansible-playbook -i inventory playbook.yml
================================================================================================================================================================================
#Ansible Inventory Update.
#Create an ini type Ansible inventory file /home/thor/playbook/inventory on jump host.
#Add App Server 1 in this inventory along with required variables that are needed to make it work.
#The inventory hostname of the host should be the server name as per the wiki, for example stapp01 for app server 1 in Stratos DC.


//SOLUTION:
#Navigate to the /home/thor/playbook/ directory on the Jump Server.
cd /home/thor/playbook/
vi inventory
- Add the below line into the in file and save and exit.
stapp01 ansible_host=ip-address ansible_ssh_pass=password ansible_user=username
#Apply playbook.
ansible-playbook -i inventory playbook.yml
========================================================================================================================================================================
#Set Limits for Resources in Kubernetes.
#Create a pod named httpd-pod and a container under it named as httpd-container.
#Use "httpd" image with latest tag only and remember to mention tag i.e "httpd:latest" and set the following limits:
- Requests: Memory: 15Mi, CPU: 100m
- Limits: Memory: 20Mi, CPU: 100m


//SOLUTION
#Create a mainfest file to execute the task according to task requirments
- Sample of the file can be found in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Set%20Limits%20in%20Kuber%20Cluster.yaml
#Apply the file in the kubernetes cluster.
kubectl apply -f filename.yaml
#Verify if pod is runninng.
kubectl get pods
============================================================================================================================================================================
#Rolling Updates in Kubernetes.
#The Nautilus DevOps team has created an image nginx:1.18 with the latest changes.
#Perform a rolling update for this application and incorporate "nginx:1.18" image.
#The deployment name is "nginx-deployment", also make sure all pods are up and running after the update.


//SOLUTION
#Update the Deployment YAML with the new nginx:1.18 image.
#If you are not sure where the YAML file is located you can modify the deployment by running:
kubectl edit deployment nginx-deployment
#This will open the YAML file in your default editor. Look for the "spec.template.spec.containers" section and update the image.
- This is what the section in text editor looks like.
spec:
  containers:
    - name: nginx-container
      image: nginx:1.17
      ports:
        - containerPort: 80

#Save the changes and close the editor.
#Ensure that the deployment is successful and all pods are running.
kubectl rollout status deployment/nginx-deployment
kubectl get pod
#If you can't find the existing YAML file for the deployment you can recreate it using the following command:
kubectl create deployment nginx-deployment --image=nginx:1.18 --dry-run=client -o yaml > nginx-deployment.yaml
#Apply the YAML file.
kubectl apply -f filename.yaml
====================================================================================================================================================================================
#Kubernetes Time Check Pod.
#Create a pod called "time-check" in the "xfusion" namespace, this pod should run a container called "time-check", container should use the "busybox" image with latest tag i.e busybox:latest.
#Create a config map called "time-config" with the data "TIME_FREQ=9" in the same namespace.
#The time-check container should run the command: while true; do date; sleep $TIME_FREQ;done and should write the result to the location "/opt/data/time/time-check.log".
#Remember you will also need to add an environmental variable "TIME_FREQ" in the container, which should pick value from the config map "TIME_FREQ" key.
#Create a volume called "log-volume" and mount the same on "/opt/data/time" within the container.


//SOLUTION
#Navigate to the Kubernetes Utility Server and create a Kubernetes pod with a specific configuration.
- You can find the sample of the YAML mainfest in this link ---> https://github.com/mightygull/Yaml-Manifest-Files/blob/main/Kubernetes%20Time%20Check%20Pod.yaml
#Apply the YAML file after creating it.
kubectl apply -f filename.yaml
#Verify the task execution.
- Check Namespace Creation:
kubectl get namespaces
- Check ConfigMap Creation:
kubectl get configmap -n xfusion
kubectl describe configmap time-config -n xfusion
- Check Pod Creation:
kubectl get pods -n xfusion
- Check Pod Logs:
kubectl logs time-check -n xfusion
- Verify Log File:
kubectl exec -it time-check -n xfusion -- cat /opt/data/time/time-check.log
============================================================================================================================================================================
#Troubleshoot Issue With Pods in Kubernetes.
- One of the junior DevOps team members was working on to deploy a stack on Kubernetes cluster.
- Somehow the pod is not coming up and its failing with some errors. We need to fix this as soon as possible.
#There is a pod named "webserver" and the container under it is named as "httpd-container". It is using image "httpd:latest".
#Also there is a sidecar container as well named "sidecar-container" which is using "ubuntu:latest" image.
#Look into the issue and fix it, make sure pod is in running state and you are able to access the app.


//SOLUTION
#Navigate to the Kubernetes Utility Server and try finding out why the pod is failing to run.
kubectl get pods
#Or can as well check pod logs.
kubectl logs -f webserver -c httpd-container
kubectl logs -f webserver -c sidecar-container
#Kindly note the "STATUS" of the pod in this case it is "ImagePullBackOff".
#We could check image availability using "docker" command.
docker pull httpd:latest
docker pull ubuntu:latest
#But since docker is not installed in our enviroment we have to troubleshoot using kubernetes service.
#To get detailed logs of the image pulling process.
kubectl logs -f -c httpd-container -n <namespace> $(kubectl get pods -n <namespace> -l app=webserver -o jsonpath='{.items[0].metadata.name}')
#To see events related to the pod's scheduling and image pulling.
kubectl describe pod
#Observing the output, the pod image had a typo "httpd:latests" instead of "httpd:latest", so we updated the image of the pod.
kubectl set image pod/webserver httpd-container=httpd:latest
#Verify that the pod is now running.
kubectl get pods
=====================================================================================================================================================================================
